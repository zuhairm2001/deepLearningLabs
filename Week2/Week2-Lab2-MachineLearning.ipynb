{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week2-Lab2-MachineLearning.ipynb","provenance":[{"file_id":"1YRuXMNtHPnW30i_vVRM4pum1N7vsEJZT","timestamp":1553459094429}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5-NUgvYdFeMi"},"source":["# Machine Learning Basics : Part-2\n","\n","### Welcome to the 2nd Lab of 42028: Deep Learning and CNN!\n","\n","In the 2nd part of this week's Lab/Tutorial session you will be implementing a K-NN and a SVM classifier for classification!\n","\n","So lets get started!"]},{"cell_type":"markdown","metadata":{"id":"4qRDTK58GYIs"},"source":["## Task for this week:\n","\n","1. Train and test a KNN classifier \n","2. Train and test a SVM classifier"]},{"cell_type":"markdown","metadata":{"id":"0vTHGRFDhKJl"},"source":["# Scikit-Learn\n","\n","Scikit-learn is a Python-based machine learning library which includes implementation of various supervised and unsupervised algorithms. This library uses Numpy and SciPy at the backend for numerical and scientific computations."]},{"cell_type":"code","metadata":{"id":"dIO3nKDZmbsD"},"source":["\"\"\"\n","Import the required libraries\n","\"\"\"\n","import numpy as np\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","# importing a module for splitting a dataset into train, and test\n","from sklearn.model_selection import train_test_split\n","# import Knn classifier\n","from sklearn.neighbors import KNeighborsClassifier\n","# confusion metric\n","from sklearn import metrics"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k95WsrV1zpI1"},"source":["# Install mglearn library\n","!pip install mglearn\n","import mglearn"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9mpVva6HoNoI"},"source":["## K-Nearest Neighbors Classification\n","\n","**Recap:**\n","* A simple supervised learning algorithm.\n","\n","* Can be used for both classification and regression\n","\n","* Non-parametric: doesn’t make any assumption on the data distribution\n","\n","* Training data is retained to make future predictions \n","\n","**How does it work?**\n","1. Computes distance between the new sample and all training samples\n","2. Distance measure: Euclidean, Manhattan etc.\n","3. Picks ‘k’ entries in the training set which are closest to the new sample\n","4. Majority voting decides the class of the new sample\n","\n","\n","<img src='http://drive.google.com/uc?export=view&id=1R7MSX-8V6SIwSD-_AgkN1als9hAKXSa7' alt='KNN'>\n","\n","Quick Reference: https://scikit-learn.org/stable/modules/neighbors.html \n","\n","Image Source: https://towardsdatascience.com/knn-using-scikit-learn-c6bed765be75\n","\n","Let's take an example of a synthetic two-classification dataset called  **forge dataset** which is provided with the **mglearn** library. The dataset has two features as shown in the following scatter plot."]},{"cell_type":"code","metadata":{"id":"0sJjgIBkLOuh"},"source":["## Load the forge dataset dataset from mglearn library\n","X,y = mglearn.datasets.make_forge()\n","## print the shape of the dataset\n","print(X.shape)\n","print(y.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7gDgOiW_k5CZ"},"source":["mglearn.plots.plot_knn_classification(n_neighbors=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nHaIKdssnkIu"},"source":["mglearn.plots.plot_knn_classification(n_neighbors=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bxx6zpQ5NNSw"},"source":["### Step-1: Create the training and test dataset"]},{"cell_type":"code","metadata":{"id":"M2muxYFjoM2t"},"source":["# Divided the dataset into train and test\n","X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T0Is9EbLNU9b"},"source":["### Step-2: Create an instance of a K-NN model with 3 NNs"]},{"cell_type":"code","metadata":{"id":"trEE2SHOOsLU"},"source":["# Create an instance of a k-NN model with 3 nearest neighbors\n","clf=KNeighborsClassifier(n_neighbors=3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lHgqg_zhNh_0"},"source":["### Step-3: Fit the model with training data"]},{"cell_type":"code","metadata":{"id":"KqMXo5AKR6sK"},"source":["# Fit the model, clf, to the training set.\n","clf.fit(X_train,y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3yHb4IELNsEX"},"source":["### Step-4: Make a prediction on the test dataset"]},{"cell_type":"code","metadata":{"id":"5__GvqEGqpdJ"},"source":["# Calculate the predictions for y_test with the clf model\n","y_predicted=clf.predict(X_test)\n","print(\"Test set predictions: {}\".format(clf.predict(X_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WEO46sx2z_Vq"},"source":["y_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hccSVT6NN_gI"},"source":["### Step-5: Compute the accuracy of the K-NN Model"]},{"cell_type":"code","metadata":{"id":"WJNxTcanR-QE"},"source":["print(\"Test set Accuracy: {:.2f}\".format(clf.score(X_test,y_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CsR5EYn8AYPR"},"source":["print(metrics.confusion_matrix(y_test, y_predicted))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"99I5pnXFq8rM"},"source":["## Analyzing KNeighbors Classifier for better understanding\n","\n","Lets plot the K-NN decision bourndaris using different values of K, such as 1, 3 , 5 and 9, for better understanding."]},{"cell_type":"code","metadata":{"id":"6UFaVcMxSaRR"},"source":["fig, axes = plt.subplots(1, 4, figsize=(10, 4))\n","\n","for n_neighbors, ax in zip([1, 3, 5, 9], axes):\n","    # the fit method returns the object self, so we can instantiate\n","    # and fit in one line\n","    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)\n","    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)\n","    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n","    ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\n","    ax.set_xlabel(\"feature 0\")\n","    ax.set_ylabel(\"feature 1\")\n","axes[0].legend(loc=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wYSZZLWErYWv"},"source":["## Task 1: Train and Test a KNN classifier on Breast Cancer dataset\n","\n","The Breast Cancer dataset contains clinical measurements of breast cancer tumor. The tumor is either \"Benign(harmless)\" or \"Malignant (harmful).\" The task is to train the classifier to predict a class (Benign Vs. Malignant ) for tumor based on its measurement. This dataset contains 569 examples, each with 30 features.\n","\n","Complete the code given below:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"0NeC-KI-rXER"},"source":["# import a breast cancer dataset\n","from sklearn.datasets import load_breast_cancer\n","# load the dataset similar to the example above\n","cancer= load_breast_cancer()\n","\n","# Examin the keys of the loaded cancer dataset\n","\n","cancer.keys()\n","\n","# print the shape of the data\n","cancer.data.shape\n","\n","# how many are malignant and how many are benign\n","\n","print(\"Sample counts for class:\\n{}\".format({n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}))\n","\n","### START YOUR CODE FROM HERE ###\n","## 1. Split the dataset into train and test sets (~ 1 line)\n","## Hint: use train_test_split(), random_state=66\n","X_train, X_test, y_train, y_test = \n","\n","## 2. build the model(~ 2 line)\n","## 2.1 Create an instance of the KNN classifier with k=3\n","clf = \n","\n","## 2.2 Fit the model to the training dataset\n","\n","\n","## 3. Calculate the train set accuracy (~ 1 line)\n","## Hint use clf.score()\n","acc_train = \n","print('Train set accuracy: ', acc_train)\n","\n","\n","## 4. Calculate the test set accuracy  (~ 1 line)\n","## Hint use clf.score()\n","acc_test = \n","print('Test set accuracy: ', acc_test)\n","\n","### END OF CODE ###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"erziV6t-_jS9"},"source":["y_test"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jLmS-Sl8lCg5"},"source":["# Linear SVM Vs. Logistic Regression\n","\n","**Goal:** Given a binary classification problem, the goal to fit the best line to the training data and have a maximum probability of classifying unseen data. \n","\n","There are two most common algorithms for  classification namely Logistic Regression and Support Vector Machines (SVM). Both these algorithms are implemented in  linear_model.LogisticRegression and svm.LinearSVC respectively. \n","\n","In logistic regression, we take the output of the linear function and squash the value within the range of [0,1] using the sigmoid function( logistic function). The Sigmoid-Function is an S-shaped curve that can take any real-valued number and map it into a value between the range of 0 and 1, but never exactly at those limits. Typically, if the squashed value is greater than a threshold value we assign it a label 1, else we assign it a label 0. This justifies the name ‘logistic regression’.\n","Note that the difference between logistic and linear regression is that Logistic regression gives you a discrete outcome but linear regression gives a continuous outcome.\n","\n","**Note:** Logistic regression is a classification algorithm and it should not be confused with Linear regression.\n","\n","**Reference**: https://towardsdatascience.com/support-vector-machine-vs-logistic-regression-94cc2975433f\n"]},{"cell_type":"markdown","metadata":{"id":"gGWUCltIWHFz"},"source":["### Example-1: Train and test Linear SVM and Logistic Regression on Forge Dataset\n"]},{"cell_type":"code","metadata":{"id":"rG-Bie4STnL-"},"source":["## Import the library for Logistic Regression and Linear SVM\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import LinearSVC\n","\n","## Load the forge dataset from mglearn library\n","X, y = mglearn.datasets.make_forge()\n","\n","## Create subplot for displaying the resuls\n","fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n","\n","## Create model for Linear SVM and Logistic Regression\n","for model, ax in zip([LinearSVC(), LogisticRegression()], axes):\n","    clf = model.fit(X, y)\n","    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\n","                                    ax=ax, alpha=.7)\n","    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n","    ax.set_title(clf.__class__.__name__)\n","    ax.set_xlabel(\"Feature 0\")\n","    ax.set_ylabel(\"Feature 1\")\n","axes[0].legend()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6BSA_9aMrjh0"},"source":["Both the model uses L2 regularization. The trade-off parameter C determine the strength of the regularization. A higher level of C corresponds to less regularizations"]},{"cell_type":"code","metadata":{"id":"iyVplz94iQiR"},"source":["mglearn.plots.plot_linear_svc_regularization()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H-f2HI03WXOz"},"source":["### Example-2: Train and test a Logistic regression on Breast Cancer Dataset"]},{"cell_type":"code","metadata":{"id":"7FXbv81liRcd"},"source":["## Apply Logistic Regression on Breast Cancer Dataset\n","## 1. Load the dataset\n","cancer = load_breast_cancer()\n","\n","## 2. Create train and test datasets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    cancer.data, cancer.target, random_state=42)\n","\n","## 3. Create a Logistic Regression classifier\n","logreg = LogisticRegression().fit(X_train, y_train)\n","\n","## 4. Evaluate the accuracy of the trained model\n","print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n","print(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WkpGBckvivf6"},"source":["## Task 2. Apply SVM with linear kernel to the breast cancer dataset."]},{"cell_type":"code","metadata":{"id":"5B-9XltUiZKH"},"source":["## Load and display the features of the Breast Cancer Dataset\n","cancer = load_breast_cancer()\n","\n","print(\"Features: \", cancer.feature_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VlFHM1jfjGCx"},"source":["## Display the target labels\n","print(\"Labels: \", cancer.target_names)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1FUgPLWRjSyk"},"source":["# print the cancer data features (top 5 records)\n","#\n","print(cancer.data[0:5])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jCAcP4-JjbeG"},"source":["print(cancer.target[0:5])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GvYS6gdjjie8"},"source":["## START YOUR FROM HERE ## \n","# 1. Split the dataset in to 30 percent testing and 70 percent training data (~ 1 line)\n","# Hint: use train_test_split() with random_state=109\n","X_train, X_test, y_train, y_test =  # 70% training and 30% test\n","## END YOUR CODE ## "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-7WuV_dxjo9R"},"source":["## START YOUR FROM HERE ## \n","#Import svm model\n","from sklearn import svm\n","\n","## 2. Create a svm Classifier (~1 line)\n","## Hint: use svm.SVC, with kernel parameter as 'linear'\n","clf =  # Linear Kernel\n","\n","## 3. Train the model using the training sets (~ 1 line)\n","## Hint: use clf.fit()\n","\n","\n","## 4. Predict the response for test dataset (~ 1 line)\n","## Hint: use clf.predict()\n","y_pred = \n","\n","## END YOUR CODE ##\n","\n","#Import scikit-learn metrics module for accuracy calculation\n","from sklearn import metrics\n","\n","# Model Accuracy: how often is the classifier correct?\n","print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WYfpr-vmjuzU"},"source":["# Model Precision: what percentage of positive tuples are labeled as such?\n","print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n","\n","# Model Recall: what percentage of positive tuples are labelled as such?\n","print(\"Recall:\",metrics.recall_score(y_test, y_pred))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vk_ZhtPPj-kX"},"source":["# Introduction to Tensorflow"]},{"cell_type":"markdown","metadata":{"id":"FD7iLaUmxL-D"},"source":["TensorFlow is a library developed by the Google Brain Team to accelerate machine learning and deep neural network research.\n","In Tensorflow, all the computations involve tensors. A tensor is a vector or matrix of n-dimensions that represents all types of data. In TensorFlow, all the operations are conducted inside a graph. The graph is a set of computation that takes place successively. Each operation is called an op node and are connected to each other.\n"]},{"cell_type":"markdown","metadata":{"id":"JZ-FZ2Dn1OsA"},"source":["Let 's define the X_1 and X_2 input nodes. When we create a node in Tensorflow, we have to choose what kind of node to create. The X1 and X2 nodes will be a placeholder node. The placeholder assigns a new value each time we make a calculation. "]},{"cell_type":"code","metadata":{"id":"M9qvxirE1SJk"},"source":["# import tensorflow\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A3oIMSOJ1vco"},"source":["### Step 1: Define the variables."]},{"cell_type":"code","metadata":{"id":"quOLsPe-1fW6"},"source":["X_1 = tf.placeholder(tf.float32, name = \"X_1\")\n","X_2 = tf.placeholder(tf.float32, name = \"X_2\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2C44azl-13Xq"},"source":["### Step 2: Define the computation"]},{"cell_type":"code","metadata":{"id":"k6YSd1gu1lZV"},"source":["multiply = tf.multiply(X_1, X_2, name = \"multiply\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8pxkcjsm2JjL"},"source":["### Step 3: Execute the operation"]},{"cell_type":"code","metadata":{"id":"If73zHWl1-tC"},"source":["with tf.Session() as session:\n","    result = session.run(multiply, feed_dict={X_1:[1,2,3], X_2:[4,5,6]})\n","    print(result)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wkREQMJQ2cs8"},"source":[""],"execution_count":null,"outputs":[]}]}